# Atlas SDK Autodiscovery

Atlas now supports an onboarding flow for self-managed, stateful agents. You can go from installation to a working `atlas run` session in three commands:

```bash
pip install arc-atlas
atlas env init
atlas run --task "Investigate the incident summary"
```

Before you run the CLI, make sure your project's own dependencies (database drivers, API keys, etc.) are installed alongside `arc-atlas`. Discovery is side-effect safe, but the follow-up validation step needs those services online.

What `atlas env init` does:

- scans for `@atlas.environment` / `@atlas.agent` classes and analyses how they interact
- if no Atlas-ready classes are found, harvests README snippets, dependency metadata, and entry points, then asks the LLM to synthesise lightweight wrapper classes plus their factories
- writes the resulting factories to `.atlas/generated_factories.py`, notes the selected runtime mode (`capabilities.preferred_mode` + `orchestration.forced_mode`), and records preflight guidance in `.atlas/discover.json`
- sets `ATLAS_DISCOVERY_VALIDATE=0` and auto-skips the sample run when prerequisites are missing; rerun `atlas env init` once dependencies are satisfied to regenerate and validate factories and capture the baseline trajectory

Wrapper synthesised by the LLM are tagged as `auto_wrapped` in `discover.json` and live alongside the other factories. They default to auto-skip until you explicitly validate them.

Once discovery succeeds, `atlas run` reuses the cached metadata, verifies that the source files still match the recorded hashes, and executes the agent loop while streaming telemetry into `.atlas/runs/`.

The generated files include:

- `.atlas/generated_factories.py` – auto-generated factories for constructor arguments that are discovered via the LLM synthesizer
- `.atlas/discover.json` – module hashes, inferred handshake, schema hints, telemetry samples, and reward observations from discovery
- `.atlas/runs/` – JSON artefacts storing telemetry and reward traces for each `atlas run` execution

If you change your environment or agent implementation, re-run `atlas env init` to refresh the cached metadata before calling `atlas run` again. The [Agent Quickstart](docs/sdk/quickstart.mdx) walks through the full onboarding flow and highlights how to validate the generated factories.

Once telemetry is flowing into Postgres, use the `scripts/report_learning.py` helper (documented in [Learning Evaluation](docs/evaluation/learning_eval.md)) to compare learning trajectories. It stitches together `sessions`, `trajectory_events`, and `discovery_runs` via `learning_key` so you can track reward deltas and adaptive mode shifts over time.

## Leaner paired-mode validation

Paired mode now ships with a compact `validation_request` envelope. Heavy fields—artifacts, full structured outputs, and prior
step transcripts—are stored once per session inside the execution context and validation requests now include each blob’s
content alongside a stable digest. When a retry produces the same structured output, the orchestrator reuses the cached verdict instead of sending the
teacher back to the LLM. Telemetry exports are unchanged: they still record the full structured output, validation result,
and `cached` provenance flag so the learning harness continues to receive identical signals.
