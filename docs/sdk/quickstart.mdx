# Stateful Agent Quickstart

This guide shows how to onboard a self-managed, stateful agent into Atlas using
the new `atlas env init` discovery workflow.

## 1. Install the SDK

```bash
pip install arc-atlas
```

Ensure your project exports an environment and an agent. The
`examples/agent.py` module in this repository demonstrates the
minimal shape:

```python
from atlas import agent, environment
from atlas.sdk.interfaces import DiscoveryContext, TelemetryEmitterProtocol

@environment
class CounterEnvironment:
    def reset(self, task: str | None = None):
        ...

    def step(self, action):
        ...

@agent
class CounterAgent:
    def plan(self, task, observation, *, emit_event=None):
        ...

    def act(self, context: DiscoveryContext, *, emit_event=None):
        ...

    def summarize(self, context: DiscoveryContext, *, history=None, emit_event=None):
        ...
```

Decorators make autodiscovery explicit. When they are absent, the CLI falls back
to heuristics (looking for `reset/step/close` on the environment and
`plan/act/summarize` on the agent). If the selected classes require constructor
arguments, `atlas env init` now gathers context from your repository, calls an
LLM (Gemini 2.5 Flash or Claude Haiku 4.5), and materialises helper factories
under `.atlas/generated_factories.py`. These helpers merge discovered defaults
with any `--env-arg/--agent-arg` overrides and defer heavyweight side effects
until validation, so the initial scan remains safe. Keep this module under
source control; it will be regenerated whenever you rerun discovery.

## 2. Discover the environment & agent

Run the discovery command from the project root:

```bash
atlas env init --task "Investigate the counter task"
```

The CLI walks the project, prompts if multiple candidates are found, spins up a
sandbox worker process, synthesises any missing factories, and runs a discovery
loop. It writes two artefacts under
`.atlas/`:

- `discover.json` captures module hashes, inferred handshake, schema hints,
  telemetry samples, and reward statistics.
- `generated_config.yaml` records the discovered pair (`behavior: self`) so the
  runtime can load them without additional configuration.

When synthesis or preflight diagnostics flag missing dependencies, discovery
marks `preflight.auto_skip = true`, flips `ATLAS_DISCOVERY_VALIDATE` to `0`, and
locks the runtime into the recommended mode. The metadata exposes the choice via
`capabilities.preferred_mode` while `.atlas/generated_config.yaml` pins
`orchestration.forced_mode` so later `atlas run` calls route directly to `auto`
or `paired` once you validate.

If the scan finds no viable environment or agent definitions, the CLI gathers
repository hints (README excerpts, entrypoints, dependency lists) and asks the
LLM to mint lightweight wrapper classes plus their factories. These generated
wrappers are tagged as `auto_wrapped` in `.atlas/discover.json`, default to
auto-skip, and inherit the same validation flow once dependencies are in place.

By default the command also performs a full sample run and records the first
trajectory under `.atlas/runs/` for continual learning. Disable this behaviour
with `--skip-sample-run`.

### Adapting to existing stacks (LangGraph, custom frameworks, …)

Most production agents already expose factory helpers or require constructor
arguments. Pass those hints directly to `atlas env init`:

```bash
atlas env init \
  --task "Investigate incident 38" \
  --env-fn langgraph_adapter:create_environment \
  --agent-fn langgraph_adapter:create_agent \
  --env-arg dataset=incidents \
  --agent-config configs/langgraph_agent.yaml
```

Use `atlas env scaffold --template langgraph` to drop the starter template into your
repository, then point the CLI at the generated factories. Key flags (see
`examples/langgraph_adapter.py` for a factory template you can adapt):

- `--env-fn / --agent-fn` – module-qualified factories that Atlas calls instead
  of the default zero-argument constructors.
- `--env-arg / --agent-arg` – repeatable `KEY=VALUE` entries forwarded to the
  factories (merge with `--env-config` / `--agent-config` when you already have
  a JSON or YAML config file).
- `--no-run` – only analyse the project; do not execute the discovery loop.

The CLI prints a summary of what it detected and, when it sees a heavy stack
such as LangGraph or DeepAgents, surfaces preflight notes (e.g., "start worker
services before running discovery"). When LLM-generated factories warn that
dependencies are offline, Atlas automatically sets `ATLAS_DISCOVERY_VALIDATE=0`,
skips the sample run, and records the notes in `.atlas/discover.json`. Re-run
`atlas env init` once services are healthy to regenerate and validate factories;
the forced mode flag will remain until you refresh discovery.

## 3. Execute tasks with the cached metadata

Once discovery is complete, you can launch the agent loop without touching
`AdapterCapabilities` or prompt plans:

```bash
atlas run --task "Summarise the telemetry captured during discovery"
```

`atlas run` validates the stored module hashes before executing. If the source
files changed, the command errors and asks you to run `atlas env init` again so
the metadata stays in sync.

Preflight notes recorded during discovery are replayed before the run so you can
double-check that the required services are online. If you deferred execution
during discovery, re-run `atlas env init` once the environment is ready, then call
`atlas run` to capture the first telemetry-rich trajectory.

Every runtime invocation persists telemetry and reward traces to
`.atlas/runs/*.json`, giving the learning eval pipeline the "what/how/why" data
it needs.

### Paired-mode validation payloads

When you run `atlas run` in paired mode, the teacher now exchanges a compact
`validation_request` schema with the orchestrator. The payload highlights step
metadata, current guidance, and hashes of large artefacts, and each request
includes the blob content keyed by those digests so the teacher still inspects the
full submission. Identical retries reuse the cached verdict instantly and mark the
corresponding telemetry event with `"cached": true`, so reward gating and the
evaluation harness continue to observe the same structured signals without incurring
extra LLM calls.

## 4. Evaluate learning progress (no hints required)

With Postgres enabled (`STORAGE__DATABASE_URL`), the runtime writes each session to the `sessions` table—including
`learning_key`, reward stats, adaptive mode history, and trajectory events. Run the evaluation helper whenever you want
to compare recent sessions against historical baselines:

```bash
python scripts/eval_learning.py \
  --database-url postgresql://atlas:atlas@localhost:5433/atlas \
  --recent-window 10 \
  --baseline-window 50
```

The script emits Markdown/JSON digests per learning key in `results/learning/` and cross-links relevant discovery runs.
See `docs/learning_eval.md` for the full workflow.
