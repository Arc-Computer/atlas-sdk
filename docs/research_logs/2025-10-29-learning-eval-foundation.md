# Learning Evaluation Foundation – Research Log

## 2025-10-29

- 10:00 EDT — Reviewed `docs/runtime_eval.md` to refresh the dual-agent evaluation harness goals, dataset structure, and learning registry integration. Key reminders: learning state is injected via `learning` config block; evaluation emits reward/latency metrics and expects pamphlet guidance hooks.
- 10:07 EDT — Studied `docs/learning_eval.md` (pre-rename) to capture current terminology (“policy nuggets”) and telemetry workflow. Noted existing rubric gates (actionability, cue presence, generality), usage tracking toggles, and report outputs (usage metrics, efficiency snapshot) that will need renaming and augmentation.
- 10:18 EDT — Read `Continual Learning Online Adaptation.tex` to align on definitions: adaptive efficiency = higher success with lower token cost over time; cross-incident transfer = guidance reused on new incidents; differentiation vs. reinforcement tied to failure avoidance vs. reuse. Memo also highlights reward/token trajectories and incident IDs as essential signals.
- 10:42 EDT — Parsed Issue #82 (“Evaluate Learning Synthesizer Quality & Meta-Prompt Redesign”). Success criteria: enforce schema/rubric with provenance, instrument runtime for cue/adoption telemetry, document evaluation workflow, and prepare research plan for prompt/model benchmarking. Dependencies: relies on runtime instrumentation (usage metrics) and report surfacing—shared with #91.
- 10:48 EDT — Parsed Issue #87 (“Rename policy nuggets to learning playbook entries”). Success criteria: rename modules/APIs to “playbook entries”, update metadata keys with backwards compatibility, refresh evaluation outputs/tests/docs, and verify reports + smoke tests. Depends on baseline artifacts pre-rename and on usage metrics introduced for #82/#91.
- 10:53 EDT — Parsed Issue #91 (“Add impact metrics for learning playbook evaluation”). Success criteria: capture reward/token deltas, incident IDs, transfer/failure signals per playbook entry; aggregate adoption/impact metrics in reports; extend tests/docs accordingly. Builds on instrumentation from #82 and must land after/beside the rename in #87 so terminology matches.
- 11:12 EDT — Ran `pytest` (108 passed, 161.41s) to establish a clean baseline prior to renames/metric work; no failures observed.
- 11:20 EDT — Inspected `results/learning/20251028T1600Z_gemini_flash_v0` baseline artifacts; confirmed current report keys use `policy_metrics`/`lifecycle_summary` with null payloads, establishing reference to update during rename.
- 11:31 EDT — Renamed `atlas/learning/nuggets.py` → `atlas/learning/playbook_entries.py`, updated synthesizer/imports/tests/configs to emit/store `playbook_entries`, and refreshed docs (`docs/learning_eval.md`, configs) plus evaluation outputs (`playbook_metrics`, `playbook_lifecycle_summary`).

- 11:38 EDT — Completed rename sweep: updated prompts/config/docs/tests to use playbook entry terminology and reran unit suite (`pytest` 108 passed, 157.35s) confirming clean break from legacy naming.
- 11:45 EDT — Removed legacy `policy_*` fallbacks across synthesizer, evaluation, usage tracker, and CLI flags to keep the playbook entry rename minimal; updated docs/tests accordingly prior to proceeding to impact metrics.
- 12:02 EDT — Re-read `docs/learning_eval.md` and `docs/runtime_eval.md` with post-rename lens; confirmed evaluation harness currently reports usage/efficiency at session granularity only. Attempted to access `docs/Continual Learning Online Adaptation.tex` but file is absent in repo—need alternate source before codifying adaptive-efficiency references.
- 12:08 EDT — Captured cross-issue outcomes: #82 still needs meta-prompt evaluation plan, schema/rubric verification post-rename, and experiment configs; #91 requires instrumentation for reward/token deltas, transfer tracking, failure avoidance signals, plus aggregation/reporting/tests/docs updates. Dependency: #91’s metrics build on the instrumentation hooks from #82 and must align with renamed playbook entry schema. Success criteria: per-entry impact metrics round-trip from runtime → persistence → reports with tests covering both modules. Open research questions: (1) precise definition of “reward delta” (session baseline vs. control window), (2) how to compute token deltas when cue hits overlap other interventions, (3) what constitutes cross-incident reuse identifier (incident id vs. normalized task slug), (4) feasible proxy for failure avoidance without new trajectory labels, and (5) replacement source for adaptive-efficiency definitions in absence of the TeX memo.
- 12:18 EDT — Ran `pytest` baseline (108 passed, 188.64s wall, 6 warnings) to confirm the repo is green before adding impact metrics instrumentation.
- 12:26 EDT — Reviewed `results/learning/index.json` and `ec8d272e307794a2_summary.json`; confirmed baseline reports lack per-entry data (`policy_metrics`, `lifecycle_summary`, `usage_metrics` all `null`), efficiency block only tracks aggregate reward/tokens without cue hits, and session payloads omit `learning_usage`. Sets reference gaps for impact metrics and reinforces need to rename keys when populating playbook-oriented sections.
- 12:34 EDT — Extended runtime instrumentation: augmented `LearningUsageTracker` with session outcome capture (reward/token snapshots, incident ids, retry/failure signals), wired `_collect_session_insights` to persist session metrics and merge per-entry impact rolls into `learning_state`, and carried impact totals forward in the synthesizer. Added unit coverage for tracker deltas/failure stats and smoke tests for impact Markdown output.
- 12:46 EDT — Updated `docs/learning_eval.md` with the new impact metric glossary, output descriptions, and suggested experiment suite; introduced prompt/model configs (`learning_overhaul_scope_shift.yaml`, `learning_overhaul_claude.yaml`) to cover transfer-focused and Claude synthesiser sweeps, outlining comparison criteria (adoption rate, reward/token deltas, impact score).
- 12:59 EDT — Re-ran full `pytest` (109 passed, 130.79s) validating instrumentation, reporting, and config additions without regressions.
- 13:05 EDT — Unable to rerun `scripts/eval_learning.py` locally due to missing Postgres session store; documented follow-up to execute the sweep once telemetry is available (use new configs + compare `playbook_impact` across variants).
- 13:14 EDT — Ran `python scripts/eval_learning.py --summary-only` against the local Gemini evaluation DB (results/learning/index.json now lists five keys). Existing artifacts lack populated `playbook_impact` because prior sessions were captured before the new instrumentation; fresh telemetry via Gemini runs is required to exercise the impact pipeline end-to-end. Logged Anthropic/Claude gap as blocked by issue #92.
- 13:16 EDT — Attempted to capture a fresh Gemini baseline session via `atlas run --config configs/eval/learning_overhaul_base.yaml --task "Summarise telemetry captured during Gemini baseline evaluation"`. Run failed immediately with `openai adapter request failed` (LiteLLM ContextWindowExceededError surfaced because the adapter still serialises full metadata). No new impact data generated; need adapter trimming fix (#92) or an offline mock before additional sessions can be recorded.
