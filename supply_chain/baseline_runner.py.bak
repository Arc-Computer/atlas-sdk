"""Lightweight baseline runner using a single LLM call per scenario."""

from __future__ import annotations

import argparse
import json
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List

from openai import OpenAI

from datetime import datetime, timezone
import os


def _load_env_api_key() -> str | None:
    env_key = os.getenv("OPENAI_API_KEY")
    if env_key:
        return env_key
    env_file = Path(".env")
    if env_file.exists():
        for raw_line in env_file.read_text().splitlines():
            line = raw_line.strip()
            if not line or line.startswith("#"):
                continue
            if line.startswith("EXPORT "):
                line = line[len("EXPORT ") :].strip()
            if "=" not in line:
                continue
            name, value = line.split("=", 1)
            if name.strip() != "OPENAI_API_KEY":
                continue
            cleaned = value.strip().strip('"').strip("'")
            if cleaned:
                os.environ.setdefault("OPENAI_API_KEY", cleaned)
                return cleaned
    return None


def _build_client(api_key: str | None) -> OpenAI:
    key = api_key or _load_env_api_key()
    if not key:
        raise RuntimeError(
            "OPENAI_API_KEY is not set. Export it, add it to .env, or pass --api-key when running the baseline."
        )
    return OpenAI(api_key=key)


@dataclass
class BaselineResult:
    scenario_id: str
    timestamp: str
    execution_time_seconds: float
    tokens: Dict[str, int | None]
    model: str
    answer: str
    status: str

    def to_dict(self) -> Dict[str, Any]:
        return {
            "scenario_id": self.scenario_id,
            "timestamp": self.timestamp,
            "execution_time_seconds": self.execution_time_seconds,
            "tokens": self.tokens,
            "model": self.model,
            "answer": self.answer,
            "status": self.status,
        }


def load_scenarios(path: str) -> List[Dict[str, Any]]:
    payload = json.loads(Path(path).read_text())
    return payload if isinstance(payload, list) else [payload]


def build_prompt(task: str, context: Dict[str, Any]) -> str:
    context_json = json.dumps(context, indent=2)
    return f"{task}\n\nContext:\n{context_json}"


def run_baseline(
    scenarios_path: str,
    output_path: str,
    limit: int | None,
    model: str,
    api_key: str | None,
) -> None:
    scenarios = load_scenarios(scenarios_path)
    if limit:
        scenarios = scenarios[:limit]

    client = _build_client(api_key)
    results: List[BaselineResult] = []
    for index, scenario in enumerate(scenarios, start=1):
        scenario_id = scenario.get("scenario_id", f"scenario_{index}")
        task = scenario.get("task", "")
        context = scenario.get("context", {})

        prompt = build_prompt(task, context)
        start = time.time()
        try:
            response = client.responses.create(
                model=model,
                input=prompt,
                reasoning={"effort": "medium"},
            )
            answer_text = getattr(response, "output_text", None)
            if not answer_text:
                fragments: List[str] = []
                for item in getattr(response, "output", []) or []:
                    for part in getattr(item, "content", []) or []:
                        text_value = getattr(part, "text", None)
                        if isinstance(text_value, str):
                            fragments.append(text_value)
                        elif isinstance(text_value, dict):
                            maybe_text = text_value.get("text")
                            if isinstance(maybe_text, str):
                                fragments.append(maybe_text)
                answer_text = "\n".join(fragments).strip() or json.dumps(response.model_dump(), indent=2)

            usage = getattr(response, "usage", None)
            prompt_tokens = getattr(usage, "prompt_tokens", None) if usage else None
            completion_tokens = getattr(usage, "completion_tokens", None) if usage else None
            total_tokens = getattr(usage, "total_tokens", None) if usage else None
            reasoning_tokens = None
            if usage:
                details = getattr(usage, "completion_tokens_details", None)
                if details is not None:
                    reasoning_tokens = getattr(details, "reasoning_tokens", None)
                    if reasoning_tokens is None and hasattr(details, "model_dump"):
                        try:
                            reasoning_tokens = details.model_dump().get("reasoning_tokens")
                        except Exception:  # pragma: no cover
                            reasoning_tokens = None
                    if reasoning_tokens is None and isinstance(details, dict):
                        reasoning_tokens = details.get("reasoning_tokens")
                    if reasoning_tokens is None and isinstance(details, list):
                        reasoning_tokens = sum(
                            int(item.get("reasoning_tokens", 0))
                            for item in details
                            if isinstance(item, dict)
                        ) or None
            tokens_payload = {
                "prompt": int(prompt_tokens) if isinstance(prompt_tokens, (int, float)) else None,
                "completion": int(completion_tokens) if isinstance(completion_tokens, (int, float)) else None,
                "total": int(total_tokens) if isinstance(total_tokens, (int, float)) else None,
                "reasoning": int(reasoning_tokens) if isinstance(reasoning_tokens, (int, float)) else None,
            }
            status = "success"
        except Exception as exc:  # pragma: no cover - network failures
            answer_text = f"Baseline run failed: {exc}"
            tokens_payload = {"prompt": 0, "completion": 0, "total": 0, "reasoning": 0}
            status = "error"
        duration = time.time() - start

        tokens_display = []
        if tokens_payload.get("prompt") is not None:
            tokens_display.append(f"{tokens_payload['prompt']:,} prompt")
        if tokens_payload.get("completion") is not None:
            tokens_display.append(f"{tokens_payload['completion']:,} completion")
        if tokens_payload.get("reasoning"):
            tokens_display.append(f"{tokens_payload['reasoning']:,} thinking")
        total_display = tokens_payload["total"] if tokens_payload.get("total") is not None else (
            (tokens_payload.get("prompt") or 0) + (tokens_payload.get("completion") or 0)
        )

        print("\n" + "─" * 80)
        print(f"Running Baseline {index}/{len(scenarios)}: {scenario_id}")
        print("─" * 80 + "\n")
        if status == "success":
            print(f"✓ Completed in {duration:.2f}s")
            token_line = "  Tokens: "
            if tokens_display:
                token_line += " + ".join(tokens_display)
                if total_display:
                    token_line += f" = {total_display:,} total"
            else:
                token_line += f"{total_display:,} total"
            if total_display:
            print(token_line + "\n")
        else:
            print(f"✗ Baseline failed in {duration:.2f}s\n  Error: {answer_text}\n")

        results.append(
            BaselineResult(
                scenario_id=scenario_id,
                timestamp=datetime.now(timezone.utc).isoformat(),
                execution_time_seconds=duration,
                tokens=tokens_payload,
                model=model,
                answer=answer_text,
                status=status,
            )
        )

    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(json.dumps([record.to_dict() for record in results], indent=2), encoding="utf-8")
    print(f"✓ Baseline results written to {output_file}")


def main() -> None:
    parser = argparse.ArgumentParser(description="Run the baseline LLM over supply-chain scenarios.")
    parser.add_argument("--scenarios", default="supply_chain/output/scenarios.json", help="Input scenarios JSON path.")
    parser.add_argument("--output", default="supply_chain/baseline_logs/baseline_results.json", help="Output JSON path.")
    parser.add_argument("--limit", type=int, default=None, help="Optional number of scenarios to run.")
    parser.add_argument("--model", default="gpt-5-mini", help="Baseline OpenAI model identifier.")
    parser.add_argument("--api-key", dest="api_key", default=None, help="Optional OpenAI API key override.")
    args = parser.parse_args()
    run_baseline(args.scenarios, args.output, args.limit, args.model, args.api_key)


if __name__ == "__main__":
    main()
