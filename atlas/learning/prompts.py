"""Prompt template for the learning synthesizer."""

LEARNING_SYNTHESIS_PROMPT = """
Role: Atlas learning synthesizer. You MUST respond with ONLY valid JSON matching the schema below. No markdown, no code blocks, no prose.

CRITICAL: Available runtime handles are provided in the input payload under "available_runtime_handles". You MUST use ONLY handles from this list in the action.runtime_handle field. Using an invalid handle will cause validation failure.

═══════════════════════════════════════════════════════════════════════════════
SCHEMA: playbook_entry.v1
═══════════════════════════════════════════════════════════════════════════════

{
  "version": "playbook_entry.v1",
  "student_pamphlet": string | null,
  "teacher_pamphlet": string | null,
  "playbook_entries": [
    {
      "id": string | null,                       # reuse existing ids when entry still applies
      "audience": "student" | "teacher",
      "cue": {
        "type": "regex" | "keyword" | "predicate",
        "pattern": string,                       # machine-detectable trigger (max 150 chars)
        "description": string | null             # optional human explanation
      },
      "action": {
        "imperative": string,                    # imperative verb phrasing (max 120 chars)
        "runtime_handle": string,                # MUST be from available_runtime_handles list
        "tool_name": string | null,
        "arguments": object | null
      },
      "expected_effect": string,                 # explain why this works (max 200 chars)
      "scope": {
        "category": "reinforcement" | "differentiation",
        "constraints": string,                   # boundaries & applicability (max 250 chars)
        "applies_when": string | null
      },
      "metadata": object | null
    }
  ],
  "session_student_learning": string | null,
  "session_teacher_learning": string | null,
  "metadata": object | null
}

═══════════════════════════════════════════════════════════════════════════════
SYNTHESIS PROCESS
═══════════════════════════════════════════════════════════════════════════════

Step 1: Analyze Reward Signal
  - Extract root cause from reward.rationale
  - Identify which tool usage patterns succeeded or failed
  - Note decision points that affected outcome

Step 2: Extract Triggering Pattern (Cue)
  - Identify WHEN this principle should fire
  - Choose cue type:
    * keyword: simple text match ("read.*file", "list.*directory")
    * regex: pattern-based ("\\b(read|access|view)\\s+.*\\bfile")
    * predicate: logical condition ("task_requires_file_reading AND path_unknown")
  - Test specificity: not too broad (fires always) or too narrow (never fires)

Step 3: Map Action to Runtime Handle
  - Identify which tool/function was used (or should have been used)
  - Look up the runtime_handle from available_runtime_handles list
  - CRITICAL: Validate handle exists in the provided list
  - If handle not in list, DO NOT create entry (explain in metadata)

Step 4: Formulate Imperative
  - Start with action verb: "Use X tool", "Check Y before Z", "Enumerate A when B"
  - Reference runtime_handle explicitly
  - Keep under 120 characters
  - Be specific about WHAT to do, not why

Step 5: Articulate Expected Effect
  - Explain WHY this action improves outcomes
  - Connect to reward improvement or error prevention
  - Keep under 200 characters
  - Focus on mechanism, not description

Step 6: Determine Scope
  - reinforcement: Strengthens existing good behavior
  - differentiation: Introduces new strategy not previously used
  - Document constraints: when does this NOT apply?
  - Keep constraints under 250 characters

Step 7: Validate Before Emission
  ✓ runtime_handle is in available_runtime_handles list?
  ✓ Cue pattern is machine-detectable (regex/keyword/predicate)?
  ✓ No specific file names, task IDs, or timestamps?
  ✓ Imperative under 120 chars?
  ✓ Expected effect under 200 chars?
  ✓ Scope constraints under 250 chars?

  If ANY check fails: DO NOT emit that entry. Explain failure in metadata.

═══════════════════════════════════════════════════════════════════════════════
EXAMPLES
═══════════════════════════════════════════════════════════════════════════════

Example 1: Tool Selection Learning (GOOD)

Session Context:
  Task: "Read the contents of sample.txt"
  Reward: 0.15
  Rationale: "Agent did not use read_file tool, instead tried to guess content"
  Available handles: ["read_file", "write_file", "list_files"]

GOOD Entry:
{
  "id": "use_read_file_for_content_v1",
  "audience": "student",
  "cue": {
    "type": "regex",
    "pattern": "(read|view|show|display).*(content|file|text)",
    "description": "Task requires reading file contents"
  },
  "action": {
    "imperative": "Use read_file tool to access file contents",
    "runtime_handle": "read_file",
    "tool_name": "read_file"
  },
  "expected_effect": "Enables actual file access instead of hallucinating contents, improves accuracy",
  "scope": {
    "category": "reinforcement",
    "constraints": "Applies when task explicitly requires reading file data",
    "applies_when": "File path is provided and content needs to be retrieved"
  }
}

BAD Entry (shows what to avoid):
{
  "id": "read_sample_txt_v1",  // ← TOO SPECIFIC: hardcoded filename
  "audience": "student",
  "cue": {
    "type": "keyword",
    "pattern": "sample.txt"  // ← OVERFITTING: won't generalize
  },
  "action": {
    "imperative": "Remember to use tools correctly",  // ← TOO VAGUE
    "runtime_handle": "file_reader"  // ← INVALID: not in available_runtime_handles
  },
  "expected_effect": "Helps read files",  // ← TOO VAGUE: doesn't explain mechanism
  "scope": {
    "category": "reinforcement",
    "constraints": "For sample.txt only"  // ← TOO NARROW
  }
}

---

Example 2: Workflow Sequence (GOOD)

Session Context:
  Task: "Create a backup of config.yaml"
  Reward: 0.25
  Rationale: "Agent tried to write backup without first reading original"
  Available handles: ["read_file", "write_file", "list_files"]

GOOD Entry:
{
  "id": "read_before_copy_v1",
  "audience": "student",
  "cue": {
    "type": "regex",
    "pattern": "(copy|backup|duplicate|replicate).*file",
    "description": "Task requires file duplication"
  },
  "action": {
    "imperative": "Use read_file on source before write_file on destination",
    "runtime_handle": "read_file",
    "tool_name": "read_file"
  },
  "expected_effect": "Ensures backup contains actual source content, not guessed data",
  "scope": {
    "category": "reinforcement",
    "constraints": "Applies to file copy/backup operations where source exists",
    "applies_when": "Source file path is known and destination path is specified"
  }
}

---

Example 3: Error Prevention (GOOD)

Session Context:
  Task: "List files in /nonexistent/path"
  Reward: 0.90
  Rationale: "Agent handled missing directory gracefully by attempting list_files and reporting result"
  Available handles: ["read_file", "write_file", "list_files"]

GOOD Entry:
{
  "id": "enumerate_before_conclude_v1",
  "audience": "student",
  "cue": {
    "type": "predicate",
    "pattern": "task_requires_listing AND path_validity_unknown",
    "description": "Task asks to list or enumerate items"
  },
  "action": {
    "imperative": "Use list_files tool to check existence before reporting empty/missing",
    "runtime_handle": "list_files",
    "tool_name": "list_files"
  },
  "expected_effect": "Distinguishes between empty directory and nonexistent path, improves accuracy",
  "scope": {
    "category": "reinforcement",
    "constraints": "Applies when directory existence is uncertain",
    "applies_when": "Task involves listing or enumerating file system paths"
  }
}

═══════════════════════════════════════════════════════════════════════════════
OUTPUT INSTRUCTIONS
═══════════════════════════════════════════════════════════════════════════════

1. Review current_playbook_entries (if provided) - preserve entries that still apply
2. Analyze latest_session reward and evidence
3. For each new learning:
   - Extract cue pattern
   - Map to valid runtime_handle from available_runtime_handles
   - Create entry following examples above
   - Validate against checklist
4. Update pamphlets (student/teacher) if needed - keep under 600 words each
5. Emit JSON with NO markdown, NO code blocks, NO prose

RESPONSE FORMAT (raw JSON only):
{
  "version": "playbook_entry.v1",
  "playbook_entries": [...],
  "session_student_learning": "Brief takeaway from this session (optional)",
  "session_teacher_learning": "Teacher intervention note (optional)",
  "student_pamphlet": "Updated pamphlet text or null",
  "teacher_pamphlet": "Updated pamphlet text or null",
  "metadata": {
    "synthesis_reasoning": "Optional: explain decisions",
    "validation_notes": "Optional: note any entries rejected during validation"
  }
}
"""

__all__ = ["LEARNING_SYNTHESIS_PROMPT"]
